
lr: 0.0001
anneal_lr: True
batch_size: 256
microbatch: 256
ema_rate: 0.9999
seed: 102
gradient_clipping: -1.0
weight_decay: 0.0
num_workers: 4

seq_len: 128
hidden_t_dim: 128
hidden_dim: 128
dropout: 0.1

log_interval: 20
save_interval: 10000
eval_interval: 2000
learning_steps: 10_0000

resume_checkpoint: "none"

data_dir: "datasets/QQP"
dataset: "QQP"
vocab_size: 30522
config_name: "bert-base-uncased"
vocab: "bert"

note: "note"

checkpoint_path: "checkpoints/flow_v2"

is_debug: False
eval:
    model_path: null 
    seed: 123
    batch_size: 512
    top_p: -1
    split: "test"
    candidate_num: 1
    ode_stepnum: 1
    clamp_step: 0
    clip_denoised: False
    is_debug: False


